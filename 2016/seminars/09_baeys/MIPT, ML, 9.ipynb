{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language ToolKit (NLTK)\n",
    "```\n",
    "   $ sudo pip install -U nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to get data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import download, download_shell, download_gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it at home\n",
    "download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BROWN CORPUS\n",
      "\n",
      "A Standard Corpus of Present-Day Edited American\n",
      "English, for use with Digital Computers.\n",
      "\n",
      "by W. N. Francis and H. Kucera (1964)\n",
      "Department of Linguistics, Brown University\n",
      "Providence, Rhode Island, USA\n",
      "\n",
      "Revised 1971, Revised and Amplified 1979\n",
      "\n",
      "http://www.hit.uib.no/icame/brown/bcm.html\n",
      "\n",
      "Distributed with the permission of the copyright holder,\n",
      "redistribution permitted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print brown.readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      The Reuters-21578 benchmark corpus, ApteMod version\n",
      "\n",
      "This is a publically available version of the well-known Reuters-21578\n",
      "\"ApteMod\" corpus for text categorization.  It has been used in\n",
      "publications like these:\n",
      "\n",
      " * Yiming Yang and X. Liu. \"A re-examination of text categorization\n",
      "   methods\".  1999.  Proceedings of 22nd Annual International SIGIR.\n",
      "   http://citeseer.nj.nec.com/yang99reexamination.html\n",
      "\n",
      " * Thorsten Joachims. \"Text categorization with support vector\n",
      "   machines: learning with many relevant features\".  1998. Proceedings\n",
      "   of ECML-98, 10th European Conference on Machine Learning.\n",
      "   http://citeseer.nj.nec.com/joachims98text.html\n",
      "\n",
      "ApteMod is a collection of 10,788 documents from the Reuters financial\n",
      "newswire service, partitioned into a training set with 7769 documents\n",
      "and a test set with 3019 documents.  The total size of the corpus is\n",
      "about 43 MB.  It is also available for download from\n",
      "http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html ,\n",
      "which includes a more extensive history of the data revisions.\n",
      "\n",
      "The distribution of categories in the ApteMod corpus is highly skewed,\n",
      "with 36.7% of the documents in the most common category, and only\n",
      "0.0185% (2 documents) in each of the five least common categories.\n",
      "In fact, the original data source is even more skewed---in creating\n",
      "the corpus, any categories that did not contain at least one document\n",
      "in the training set and one document in the test set were removed from\n",
      "the corpus by its original creator.\n",
      "\n",
      "In the ApteMod corpus, each document belongs to one or more\n",
      "categories.  There are 90 categories in the corpus.  The average\n",
      "number of categories per document is 1.235, and the average number of\n",
      "documents per category is about 148, or 1.37% of the corpus.\n",
      "\n",
      " -Ken Williams\n",
      "  ken@mathforum.org\n",
      "\n",
      "         Copyright & Notification \n",
      "\n",
      "(extracted from the README at the UCI address above)\n",
      "\n",
      "The copyright for the text of newswire articles and Reuters\n",
      "annotations in the Reuters-21578 collection resides with Reuters Ltd.\n",
      "Reuters Ltd. and Carnegie Group, Inc. have agreed to allow the free\n",
      "distribution of this data *for research purposes only*.  \n",
      "\n",
      "If you publish results based on this data set, please acknowledge\n",
      "its use, refer to the data set by the name \"Reuters-21578,\n",
      "Distribution 1.0\", and inform your readers of the current location of\n",
      "the data set (see \"Availability & Questions\").\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print reuters.readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure\n",
      "belles_lettres\n",
      "editorial\n",
      "fiction\n",
      "government\n",
      "hobbies\n",
      "humor\n",
      "learned\n",
      "lore\n",
      "mystery\n",
      "news\n",
      "religion\n",
      "reviews\n",
      "romance\n",
      "science_fiction\n",
      "\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print \"\\n\".join(brown.categories())\n",
    "print\n",
    "print len(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reuters.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The', u'Fulton', u'County', u'Grand', u'Jury', ...]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place . The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted . The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan ...\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "print \" \".join(islice(brown.words(), 100)) + \" ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sents interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'The', u'Fulton', u'County', u'Grand', u'Jury', u'said', u'Friday', u'an', u'investigation', u'of', u\"Atlanta's\", u'recent', u'primary', u'election', u'produced', u'``', u'no', u'evidence', u\"''\", u'that', u'any', u'irregularities', u'took', u'place', u'.'], [u'The', u'jury', u'further', u'said', u'in', u'term-end', u'presentments', u'that', u'the', u'City', u'Executive', u'Committee', u',', u'which', u'had', u'over-all', u'charge', u'of', u'the', u'election', u',', u'``', u'deserves', u'the', u'praise', u'and', u'thanks', u'of', u'the', u'City', u'of', u'Atlanta', u\"''\", u'for', u'the', u'manner', u'in', u'which', u'the', u'election', u'was', u'conducted', u'.'], ...]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "\n",
      "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "\n",
      "The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .\n",
      "\n",
      "`` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "\n",
      "The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in islice(brown.sents(), 5):\n",
    "    print \" \".join(sentence)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.\n",
      "\n",
      "\n",
      "\tThe/at jury/nn further/rbr said/vbd in/in term-end/nn presentments/nns that/cs the/at City/nn-tl Executive/jj-tl Committee/nn-tl ,/, which/wdt had/hvd over-all/jj charge/nn of/in the/at election/nn ,/, ``/`` deserves/vbz the/at praise/nn and/cc thanks/nns of/in the/at City/nn-tl of/in-tl Atlanta/np-tl ''/'' for/in the/at manner/nn in/in which/wdt the/at election/nn was/bedz conducted/vbn ./.\n",
      "\n",
      "\n",
      "\tThe/at September-October/np term/nn jury/nn had/hvd been/ben charged/vbn by/in Fulton/np-tl Superior/jj-tl Court/nn-tl Judge/nn-tl Durwood/np Pye/np to/to investigate/vb reports/nns of/in possible/jj ``/`` irregularities/nns ''/'' in/in the/at hard-fought/jj primary/nn which/wdt was/bedz won/vbn by/in Mayor-nominate/nn-tl Ivan/np Allen/np Jr./\n"
     ]
    }
   ],
   "source": [
    "print brown.raw()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Question: Formula?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unused = \"\"\"\n",
    "A classifier based on the Naive Bayes algorithm.  In order to find the\n",
    "probability for a label, this algorithm first uses the Bayes rule to\n",
    "express P(label|features) in terms of P(label) and P(features|label):\n",
    "\n",
    "|                       P(label) * P(features|label)\n",
    "|  P(label|features) = ------------------------------\n",
    "|                              P(features)\n",
    "\n",
    "The algorithm then makes the 'naive' assumption that all features are\n",
    "independent, given the label:\n",
    "\n",
    "|                       P(label) * P(f1|label) * ... * P(fn|label)\n",
    "|  P(label|features) = --------------------------------------------\n",
    "|                                         P(features)\n",
    "\n",
    "Rather than computing P(featues) explicitly, the algorithm just\n",
    "calculates the denominator for each label, and normalizes them so they\n",
    "sum to one:\n",
    "\n",
    "|                       P(label) * P(f1|label) * ... * P(fn|label)\n",
    "|  P(label|features) = --------------------------------------------\n",
    "|                        SUM[l]( P(l) * P(f1|l) * ... * P(fn|l) )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names Corpus, Version 1.3 (1994-03-29)\n",
      "Copyright (C) 1991 Mark Kantrowitz\n",
      "Additions by Bill Ross\n",
      "\n",
      "This corpus contains 5001 female names and 2943 male names, sorted\n",
      "alphabetically, one per line.\n",
      "\n",
      "You may use the lists of names for any purpose, so long as credit is\n",
      "given in any published work. You may also redistribute the list if you\n",
      "provide the recipients with a copy of this README file. The lists are\n",
      "not in the public domain (I retain the copyright on the lists) but are\n",
      "freely redistributable.  If you have any additions to the lists of\n",
      "names, I would appreciate receiving them.\n",
      "\n",
      "Mark Kantrowitz <mkant+@cs.cmu.edu>\n",
      "http://www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/\n"
     ]
    }
   ],
   "source": [
    "print names.readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'female.txt', u'male.txt']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Aamir', u'Aaron', u'Abbey', u'Abbie', u'Abbot', u'Abbott', u'Abby', u'Abdel', u'Abdul', u'Abdulkarim']\n",
      "[u'Abagael', u'Abagail', u'Abbe', u'Abbey', u'Abbi', u'Abbie', u'Abby', u'Abigael', u'Abigail', u'Abigale']\n"
     ]
    }
   ],
   "source": [
    "print names.words(fileids=['male.txt'])[:10]\n",
    "print names.words(fileids=['female.txt'])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK is smart =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Aamir', u'Aaron', u'Abbey', u'Abbie', u'Abbot', u'Abbott', u'Abby', u'Abdel', u'Abdul', u'Abdulkarim']\n"
     ]
    }
   ],
   "source": [
    "print names.words('male.txt')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://www.nltk.org/images/supervised-classification.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"http://www.nltk.org/images/supervised-classification.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = (\n",
    "    [(name, 'male') for name in names.words('male.txt')] +\n",
    "    [(name, 'female') for name in names.words('female.txt')]\n",
    ")\n",
    "shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Luella', 'female'),\n",
       " (u'Arlee', 'female'),\n",
       " (u'Bree', 'female'),\n",
       " (u'Sibylla', 'female'),\n",
       " (u'Gretal', 'female'),\n",
       " (u'Wenda', 'female'),\n",
       " (u'Jazmin', 'female'),\n",
       " (u'Curt', 'male'),\n",
       " (u'Westley', 'male'),\n",
       " (u'Amie', 'female')]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_features(text):\n",
    "    return {\n",
    "        'last_letter' : text[-1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = list((make_features(name), label) for name, label in names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_size = 500\n",
    "train_set, test_set = featuresets[test_size:], featuresets[:test_size]\n",
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['male', 'female']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "female\n"
     ]
    }
   ],
   "source": [
    "print classifier.classify(make_features('Mihkael'))\n",
    "print classifier.classify(make_features('Elizabeth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.778"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = u'a'           female : male   =     37.4 : 1.0\n",
      "             last_letter = u'k'             male : female =     31.7 : 1.0\n",
      "             last_letter = u'f'             male : female =     16.6 : 1.0\n",
      "             last_letter = u'p'             male : female =     11.9 : 1.0\n",
      "             last_letter = u'v'             male : female =     11.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_dist = classifier.prob_classify(make_features(\"Mikhael\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_log': True,\n",
       " '_prob_dict': {'female': -1.0239484849158842, 'male': -0.97644257307679}}"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dist.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studnet Question: Why log?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from string import letters\n",
    "from nltk import bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fist': 'X', 'last': 'n'}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def line(name):\n",
    "    features = {}\n",
    "    features['last'] = name[-1]\n",
    "    return features\n",
    "\n",
    "baisline(\"Xaronkn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 'X', 'vowels': 2}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def qqq(name):\n",
    "    features = {}\n",
    "    features['first'] = name[0]\n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "    features['vowels'] = len([f for f in name if f in vowels])\n",
    "    # features['last'] = name[-1]\n",
    "    return features\n",
    "\n",
    "qqq(\"Xaronkn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Xa': 1,\n",
       " 'Xar': 1,\n",
       " 'ar': 1,\n",
       " 'aro': 1,\n",
       " 'kn': 1,\n",
       " 'last1': 'n',\n",
       " 'last2': 'kn',\n",
       " 'last3': 'nkn',\n",
       " 'len111': 7,\n",
       " 'nk': 1,\n",
       " 'nkn': 1,\n",
       " 'on': 1,\n",
       " 'onk': 1,\n",
       " 'ro': 1,\n",
       " 'ron': 1}"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def line00000000(name):\n",
    "    features = {}\n",
    "    features['last1'] = name[-1:]\n",
    "    features['last2'] = name[-2:]\n",
    "    features['last3'] = name[-3:]\n",
    "    for i in range(len(name) - 1):\n",
    "    \tfeatures[name[i:i+2]] = 1\n",
    "    \tif i + 2 < len(name):\n",
    "    \t\tfeatures[name[i:i + 3]] = 1\n",
    "    features['len111'] = len(name)\n",
    "    return features\n",
    "\n",
    "line00000000(\"Xaronkn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fist': 'X', 'last': 'n'}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def baseline(name):\n",
    "    features = {}\n",
    "    features['last'] = name[-1:]\n",
    "    features['last2'] = name[-2:]\n",
    "    features['last3'] = name[-3:]\n",
    "    return features\n",
    "\n",
    "baisline(\"Xaronkn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'X', '2': 'Xa', 'one': 'n', 'three': 'nkn', 'two': 'kn'}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(name):\n",
    "    res = dict()\n",
    "    res['one'] = name[-1]\n",
    "    res['two'] = name[-2:]\n",
    "    res['three'] = name[-3:]\n",
    "    res['1'] = name[0]\n",
    "    res['2'] = name[0:2]\n",
    "    return res\n",
    "f(\"Xaronkn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "teams = {\n",
    "    'arseny' : arseny,\n",
    "    'line': line,\n",
    "    'baseline': baseline,\n",
    "    'line00000000': line00000000,\n",
    "    'qqq': qqq,\n",
    "    'f': f\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "competition = []\n",
    "\n",
    "for team_name, feature_maker in teams.items():\n",
    "    featuresets = list((feature_maker(name), label) for name, label in names)\n",
    "    train_set, test_set = featuresets[test_size:], featuresets[:test_size]\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    score = accuracy(classifier, test_set)\n",
    "    competition.append((team_name, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   2last = u'na'          female : male   =     98.4 : 1.0\n",
      "                   2last = u'la'          female : male   =     73.9 : 1.0\n",
      "                   2last = u'ia'          female : male   =     40.9 : 1.0\n",
      "                    last = u'a'           female : male   =     37.4 : 1.0\n",
      "                   2last = u'sa'          female : male   =     35.0 : 1.0\n",
      "                    last = u'k'             male : female =     31.7 : 1.0\n",
      "                   2last = u'us'            male : female =     27.8 : 1.0\n",
      "                   2last = u'io'            male : female =     27.2 : 1.0\n",
      "                   2last = u'ra'          female : male   =     27.2 : 1.0\n",
      "                   2last = u'ta'          female : male   =     26.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>line00000000</td>\n",
       "      <td>0.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>arseny</td>\n",
       "      <td>0.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qqq</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Team  Score\n",
       "2  line00000000  0.834\n",
       "1             f  0.812\n",
       "5        arseny  0.806\n",
       "0      baseline  0.800\n",
       "4          line  0.778\n",
       "3           qqq  0.660"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame.from_records(competition, columns=['Team', 'Score']).sort_values(by=\"Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arseny(text):\n",
    "    features = {}\n",
    "    features['first'] = text[0]\n",
    "    features['last'] = text[-1]\n",
    "    features['2first'] = \"\".join(text[:2])\n",
    "    features['2last'] = \"\".join(text[-2:])\n",
    "    \n",
    "    pair_count = 0\n",
    "    for bigram, counts in Counter(bigrams(text)).items():\n",
    "        first, second = bigram\n",
    "        if first == second:\n",
    "            pair_count += 1\n",
    "    \n",
    "    features['pair_count'] = pair_count\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Polarity Dataset Version 2.0\n",
      "Bo Pang and Lillian Lee\n",
      "\n",
      "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
      "\n",
      "Distributed with NLTK with permission from the authors.\n",
      "\n",
      "=======\n",
      "\n",
      "Introduction\n",
      "\n",
      "This README v2.0 (June, 2004) for the v2.0 polarity dataset comes from\n",
      "the URL http://www.cs.cornell.edu/people/pabo/movie-review-data .\n",
      "\n",
      "=======\n",
      "\n",
      "What's New -- June, 2004\n",
      "\n",
      "This dataset represents an enhancement of the review corpus v1.0\n",
      "described in README v1.1: it contains more reviews, and labels were\n",
      "created with an improved rating-extraction system.\n",
      "\n",
      "=======\n",
      "\n",
      "Citation Info \n",
      "\n",
      "This data was first used in Bo Pang and Lillian Lee,\n",
      "``A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization \n",
      "Based on Minimum Cuts'',  Proceedings of the ACL, 2004.\n",
      "\n",
      "@InProceedings{Pang+Lee:04a,\n",
      "  author =       {Bo Pang and Lillian Lee},\n",
      "  title =        {A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts},\n",
      "  booktitle =    \"Proceedings of the ACL\",\n",
      "  year =         2004\n",
      "}\n",
      "\n",
      "=======\n",
      "\n",
      "Data Format Summary \n",
      "\n",
      "- review_polarity.tar.gz: contains this readme and  data used in\n",
      "  the experiments described in Pang/Lee ACL 2004.\n",
      "\n",
      "  Specifically:\n",
      "\n",
      "  Within the folder \"txt_sentoken\" are the 2000 processed down-cased\n",
      "  text files used in Pang/Lee ACL 2004; the names of the two\n",
      "  subdirectories in that folder, \"pos\" and \"neg\", indicate the true\n",
      "  classification (sentiment) of the component files according to our\n",
      "  automatic rating classifier (see section \"Rating Decision\" below).\n",
      "\n",
      "  File names consist of a cross-validation tag plus the name of the\n",
      "  original html file.  The ten folds used in the Pang/Lee ACL 2004 paper's\n",
      "  experiments were:\n",
      "\n",
      "     fold 1: files tagged cv000 through cv099, in numerical order\n",
      "     fold 2: files tagged cv100 through cv199, in numerical order     \n",
      "     ...\n",
      "     fold 10: files tagged cv900 through cv999, in numerical order\n",
      "\n",
      "  Hence, the file neg/cv114_19501.txt, for example, was labeled as\n",
      "  negative, served as a member of fold 2, and was extracted from the\n",
      "  file 19501.html in polarity_html.zip (see below).\n",
      "\n",
      "  Each line in each text file corresponds to a single sentence, as\n",
      "  determined by Adwait Ratnaparkhi's sentence boundary detector\n",
      "  MXTERMINATOR.\n",
      " \n",
      "  Preliminary steps were taken to remove rating information from the\n",
      "  text files, but only the rating information upon which the rating\n",
      "  decision was based is guaranteed to have been removed. Thus, if the\n",
      "  original review contains several instances of rating information,\n",
      "  potentially given in different forms, those not recognized as valid\n",
      "  ratings remain part of the review text.\n",
      "\t\n",
      "- polarity_html.zip: The original source files from which the\n",
      "  processed, labeled, and (randomly) selected data in\n",
      "  review_polarity.tar.gz was derived.\n",
      "\n",
      "  Specifically:  \n",
      "\n",
      "  This data consists of unprocessed, unlabeled html files from the\n",
      "  IMDb archive of the rec.arts.movies.reviews newsgroup,\n",
      "  http://reviews.imdb.com/Reviews. The files in review_polarity.tar.gz\n",
      "  represent a processed subset of these files. \n",
      "\n",
      "=======\n",
      "\n",
      "Rating Decision (Appendix A)\n",
      "\n",
      "This section describes how we determined whether a review was positive\n",
      "or negative.\n",
      "\n",
      "The original html files do not have consistent formats -- a review may\n",
      "not have the author's rating with it, and when it does, the rating can\n",
      "appear at different places in the file in different forms.  We only\n",
      "recognize some of the more explicit ratings, which are extracted via a\n",
      "set of ad-hoc rules.  In essence, a file's classification is determined\n",
      "based on the first rating we were able to identify.\n",
      "\n",
      "\n",
      "- In order to obtain more accurate rating decisions, the maximum\n",
      "\trating must be specified explicitly, both for numerical ratings\n",
      "\tand star ratings.  (\"8/10\", \"four out of five\", and \"OUT OF\n",
      "\t****: ***\" are examples of rating indications we recognize.)\n",
      "\n",
      "- With a five-star system (or compatible number systems):\n",
      "\tthree-and-a-half stars and up are considered positive, \n",
      "\ttwo stars and below are considered negative.\n",
      "- With a four-star system (or compatible number system):\n",
      "\tthree stars and up are considered positive, \n",
      "\tone-and-a-half stars and below are considered negative.  \n",
      "- With a letter grade system:\n",
      "\tB or above is considered positive,\n",
      "\tC- or below is considered negative.\n",
      "\n",
      "We attempted to recognize half stars, but they are specified in an\n",
      "especially free way, which makes them difficult to recognize.  Hence,\n",
      "we may lose a half star very occasionally; but this only results in 2.5\n",
      "stars in five star system being categorized as negative, which is \n",
      "still reasonable.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print movie_reviews.readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'neg', u'pos']"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (list(movie_reviews.words(fileid)), category)\n",
    "    for category in movie_reviews.categories()\n",
    "        for fileid in movie_reviews.fileids(category)\n",
    "]\n",
    "shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words = FreqDist(w.lower() for w in movie_reviews.words())\n",
    "# Filter most common\n",
    "word_features = all_words.keys()[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "          contains(sans) = True              neg : pos    =      9.0 : 1.0\n",
      "    contains(mediocrity) = True              neg : pos    =      7.7 : 1.0\n",
      "     contains(dismissed) = True              pos : neg    =      7.0 : 1.0\n",
      "   contains(bruckheimer) = True              neg : pos    =      6.3 : 1.0\n",
      "        contains(doubts) = True              pos : neg    =      5.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ConfusionMatrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classification = classifier.classify_many((feature_set for feature_set, label in featuresets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix = ConfusionMatrix(list(label for features, label in featuresets), classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   n   p |\n",
      "    |   e   o |\n",
      "    |   g   s |\n",
      "----+---------+\n",
      "neg |<901> 99 |\n",
      "pos | 234<766>|\n",
      "----+---------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "    |      n      p |\n",
      "    |      e      o |\n",
      "    |      g      s |\n",
      "----+---------------+\n",
      "neg | <45.0%>  5.0% |\n",
      "pos |  11.7% <38.3%>|\n",
      "----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print confusion_matrix.pretty_format(sort_by_count=True, show_percents=False)\n",
    "print confusion_matrix.pretty_format(sort_by_count=True, show_percents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
