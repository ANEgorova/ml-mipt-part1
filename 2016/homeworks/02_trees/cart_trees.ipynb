{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('kaggle_data/adult.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = df[df.columns[:-1]].values, df[df.columns[-1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def number_encode_features(df):\n",
    "    # Try to encode features with numbers\n",
    "    return result, encoders\n",
    "\n",
    "encoded_data, encoders = number_encode_features(df)\n",
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(19,8))\n",
    "cols = 5\n",
    "rows = ceil(float(encoded_data.shape[1]) / cols)\n",
    "for i, column in enumerate(encoded_data.columns):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    ax.set_title(column)\n",
    "    encoded_data[column].hist(axes=ax)\n",
    "    plt.xticks(rotation=\"vertical\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do a visialisation of one by one numerical feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree (CART)\n",
    "You could use this doc to implement (and hopefully understand) CART: ftp://public.dhe.ibm.com/software/analytics/spss/support/Stats/Docs/Statistics/Algorithms/13.0/TREE-CART.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(left, right):\n",
    "    #Implement here Gini criterion\n",
    "    return 0\n",
    "\n",
    "def twoing(left, right):\n",
    "    #Implement here Twoing criterion\n",
    "    return 0\n",
    "\n",
    "# Also you could implement ordered Twoing criterion,\n",
    "# there is at least one acceptable feature for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CART(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin):\n",
    "    def __init__(self, leaf_min_size, tree_depth, criterion='gini'):\n",
    "        self.leaf_min_size = leaf_min_size\n",
    "        self.tree_depth = tree_depth\n",
    "        self.tree = None\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        1. Find each predictor’s best split. \n",
    "        For each continuous and ordinal predictor, sort its values from the smallest to the largest.\n",
    "        For the sorted predictor, go through each value from top to examine each candidate split\n",
    "        point (call it v, if x ≤ v, the case goes to the left child node, otherwise, goes to the right.)\n",
    "        to determine the best. The best split point is the one that maximize the splitting criterion\n",
    "        the most when the node is split according to it. The definition of splitting criterion is in\n",
    "        later section.\n",
    "        For each nominal predictor, examine each possible subset of categories (call it A, if\n",
    "        x ∈ A, the case goes to the left child node, otherwise, goes to the right.) to find the best\n",
    "        split.\n",
    "        2. Find the node’s best split.\n",
    "        Among the best splits found in step 1, choose the one that maximizes the splitting\n",
    "        criterion.\n",
    "        3. Split the node using its best split found in step 2 if the stopping rules are not satisfied. \n",
    "        \n",
    "        Write your growing tree code here. You should use stopping_criterion while growing it.\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def stopping_criterion(node, current_tree_depth):\n",
    "        \"\"\"\n",
    "        There are a few stopping criteria. You should implement them here.\n",
    "        \n",
    "        • If a node becomes pure; that is, all cases in a node have identical values of the dependent\n",
    "        variable, the node will not be split.\n",
    "        • If all cases in a node have identical values for each predictor, the node will not be split.\n",
    "        • If the current tree depth reaches the user-specified maximum tree depth limit value, the\n",
    "        tree growing process will stop.\n",
    "        • If the size of a node is less than the user-specified minimum node size value, the node\n",
    "        will not be split.\n",
    "        • If the split of a node results in a child node whose node size is less than the userspecified\n",
    "        minimum child node size value, the node will not be split. \n",
    "        • If for the best split, the improvement is smaller than the user-specified minimum improvement, \n",
    "        the node will not be split. \n",
    "        \n",
    "        Some sources call some of the criteria (e.g. tree depth) a pre-pruning, so you could check sources \n",
    "        about it before implementation.\n",
    "        \"\"\"\n",
    "        return False\n",
    "    \n",
    "    def impurity(left, right):\n",
    "        if criterion == 'gini':\n",
    "            return gini(left, right)\n",
    "        elif criterion == 'twoing':\n",
    "            return twoing(left, right)\n",
    "        else:\n",
    "            #You could insert here call for ordered Twoing\n",
    "            raise ValueError(\"Unknown criterion type!\")\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict labels for test data using this classifier. Take the tree growned in the fit stage.\n",
    "        Don't forget to handle missing values: the label you choose should be from the sub-tree \n",
    "        which has less enthropy.\n",
    "\n",
    "        Inputs:\n",
    "        - X_test: A numpy array of shape (num_test, D) containing test data consisting\n",
    "                  of num_test samples each of dimension D.\n",
    "\n",
    "        Returns:\n",
    "        - y: A numpy array of shape (num_test,) containing predicted labels for the\n",
    "             test data, where y[i] is the predicted label for the test point X[i].\n",
    "        \"\"\"\n",
    "        y_pred = numpy.zeros(X_test.shape[0])\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cart = CART(10,20)\n",
    "cart = cart.fit(X_train, Y_train)\n",
    "%time y_pred = cart.predict(X_train[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional part: Pruning\n",
    "More information you could get here: http://www.dcc.fc.up.pt/~ltorgo/PhD/th4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pruning(cart_tree, alpha):\n",
    "    '''\n",
    "    CART prunes a large regression tree T using a two-stage\n",
    "    algorithm called Error-Complexity pruning. This method is\n",
    "    based on a measure of a tree called error-complexity EC(T, α), \n",
    "    which is defined as,\n",
    "    \n",
    "    EC (T, α) = Err(T) + α * #T\n",
    "    \n",
    "    where,\n",
    "    \n",
    "    Err(T) is the error rate for the tree T;\n",
    "    #T is a number of leaves of tree T;\n",
    "    and α is the complexity parameter.\n",
    "    \n",
    "    EC should decrease for each node in a tree replaced by leaf. So you go through \n",
    "    all the nodes and trying to replace them, and checking with EC as it defined above.\n",
    "    \n",
    "    Some sources refer to this as post-pruning.\n",
    "    '''\n",
    "    pruned_tree = cart_tree\n",
    "    return pruned_tree\n",
    "\n",
    "cart = CART(10,20)\n",
    "pruned = pruning(cart, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a picture of your tree, using GraphViz\n",
    "\n",
    "value = \"\"\"strict digraph G {\n",
    "1 [label=\"hi\\\\nhi\", shape=\"box\"];\n",
    "2;\n",
    "3[label=\"hi\\\\nhi\"];\n",
    "4;\n",
    "5;\n",
    "1 -> 2;\n",
    "1 -> 3;\n",
    "1 -> 5  [weight=\"0.5\", color=\"red\"];\n",
    "2 -> 4;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "open('tree.dot', 'w').write(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!dot -Tpng tree.dot -o tree.png\n",
    "Image('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree comparison visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://scikit-learn.org/stable/_images/plot_classifier_comparison_001.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Сгенерируйте 3 двумерных двухклассовых выборки\n",
    "# 2) Обучите на нах классификаторы: Ваше деврево, kNN\n",
    "# 3) С постоянным шагом по каждой координате проведите классфикацию точек плоскости\n",
    "# 4) Какой классфикатор работает лучше, в чем преимущества и недостатки Решаюших деревьев и KNN для этих выборок?\n",
    "\n",
    "    \n",
    "# --------\n",
    "#1) Используйте KNN -- http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "#2) Для генерации выборки используйте from sklearn.datasets import make_moons, make_circles\n",
    "#3) Пример решения http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "# Example:\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "display(Image(url=\"http://scikit-learn.org/stable/_images/plot_classifier_comparison_001.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree bagging\n",
    "The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set $X=x_1,...,x_n$ with responses $Y = y_1, ..., y_n$, bagging repeatedly ($ B $ times) selects a random sample with replacement of the training set and fits trees to these samples.\n",
    "After training, predictions for unseen samples $ x' $ can be made by averaging the predictions from all the individual regression trees on $x'$:\n",
    "\n",
    "$$ \\hat{f} = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}_b (x') $$\n",
    "\n",
    "or by taking the majority vote in the case of decision trees.\n",
    "\n",
    "This bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagging(X, y, size):\n",
    "    #Implement random sampling here\n",
    "    sample_X, sample_y = X[:size], y[:size]\n",
    "    return sample_X, sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Forest(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin):\n",
    "    def __init__(self, num_trees):\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        Create trees here, using bagging.\n",
    "        '''\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        Predict the label here using your grown trees.\n",
    "        '''\n",
    "        y_pred = numpy.zeros(X_test.shape[0])\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = Forest(10)\n",
    "forest = forest.fit(X_train, Y_train)\n",
    "%time y_pred = cart.predict(X_train[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code F1 score and Cross Validation Process\n",
    "You could use sklearn functions instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1\n",
    "\n",
    "def f1(y_true, y_predict):\n",
    "    score = 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "\n",
    "def cross_validation(X, y, metric, cv_fold=5):\n",
    "    scores = []\n",
    "    # Put your code hear\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#You could try to vary parameters of a tree or of a forest, or may be do something more clever with forest itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
