		\documentclass[a4paper, 12pt]{article}				

%============== Русский язык ===============================
\usepackage[T2A]{fontenc}		
\usepackage[utf8]{inputenc}	
\usepackage[english,russian]{babel}	
\usepackage{multicol}
\usepackage{minted}

%============== Всякие пакеты ===============================
% нужно поставить pygments - $ sudo pip install pygments
\usepackage{listings, minted, chngcntr, float, amsmath, amssymb, cmap, graphicx, xcolor, hyperref, geometry}

 \geometry{
 	a4paper,
 	total={210mm,297mm},
 	left=20mm,
 	right=20mm,
 	top=20mm,
 	bottom=20mm,
 }

%============== Цветные гиперссылки =========================
\definecolor{wine-stain}{rgb}{1,0,0}
\hypersetup{colorlinks, linkcolor=wine-stain, linktoc=all}

%============== Настройка листингов =========================
\usemintedstyle{default}
\definecolor{codebg}{rgb}{0.96,0.96,0.96}

\renewcommand\listoflistingscaption{Листинги}
\renewcommand\listingscaption{Листинг}

% === Это магия, чтоб все с рускими буквами хорошо в листиге было
% === Это макрос, чтоб не писать рараметры каждый раз

\makeatletter
\newenvironment{mycode}[3]
{\VerbatimEnvironment
	\minted@resetoptions
	\setkeys{minted@opt}{bgcolor=codebg, linenos=true, frame=lines, numbersep=5pt, tabsize=4}
	\renewcommand{\minted@proglang}[1]{#1}
	\begin{listing}[H]%% default placing
		\centering
		\caption{#2}\label{#3}
		\begin{VerbatimOut}{\jobname.pyg}}
		{\end{VerbatimOut}
		\minted@pygmentize{\minted@proglang{}}
		\DeleteFile{\jobname.pyg}
	\end{listing}}
	\makeatother
%================ Всякое разное форматирование ========================

\textheight=27cm 		% высота текста
\textwidth=19cm 		% ширина текста
\oddsidemargin=-1.5cm 		% отступ от левого края
\topmargin=-3.0cm 		% отступ от верхнего края
\parindent=24pt 		% абзацный отступ
\parskip=0pt 			% интервал между абзацами
\tolerance=2000	 		% терпимость к "жидким" строкам
\flushbottom 			% выравнивание высоты страниц

\parindent=0cm
\title{MIPT HW 2}
%===================================================================
%===================================================================
%===================================================================
%===================================================================

\begin{document}

  \begin{center}
    \textsc{\textbf{
    	{\Large Машинное Обучение МФТИ \\
        \vspace{0.5cm}
    	Практическое задание №2: Решающие деревья}}}
  \end{center}

\begin{center}
	%\includegraphics[scale=1.0]{mnist}
\end{center}

В качестве второй домашней работы студентам предлагается принять участие в соревновании по предсказнию уровня заработной платы на Kaggle. Для выполнения домашнего задания используйте ipython notebook с небольшими заготовками кода.

Соревнование общее -- бонусы получают три первые места в каждой учебной группе.

\subsection*{Настройка окружения, инструкция по отправке решения}
  \begin{enumerate}
	\item Вы можете загрузить \texttt{cart\_trees.ipynb} - это ноутбук с кодом, который необходимо доделать, следуя инструкциям.
    \item Установите \href{https://www.continuum.io/downloads}{Anaconda} для \textbf{Python 2.7}. 
    \item Установите \href{http://www.graphviz.org/}{GraphViz}.
     \item Сохранить ipynb с выполненным экспериментом. Нужно запаковать, переименовать \texttt{hw02\_<фамилия>.zip } и отправить в приватный канал в piazza с названием вида \texttt{HW2\_группа\_фамилия}. В ноутбуке должны быть описаны эксперименты, которые показали улучшение, и приведен код, который их выполняет. Можете описать интересные идеи, которые не сработали. 
  \end{enumerate}

\subsection*{Описание данных и метрики}
Для решения задания нужно обучить модель на обучающей выборке \texttt{adult.data}, сделать предсказание на \texttt{adult.val} и оправить свое решение на kaggle. Для ранжирования решений на \texttt{leadboard} используется метрика $F_1$~--~описание её можно посмотреть, например, здесь: \href{https://en.wikipedia.org/wiki/F1_score}{Wikipedia}. 

$$F_1 = \frac{precision + recall}{2~ precision ~recall}$$

Каждый объект в обучающей (\texttt{adult.data}) и тестовой (\texttt{adult.val}) выборках представляет собой набор из 14 значений как числовых, так и категориальных. 

\subsection*{Задания, которые нужно выполнить}
\begin{enumerate}
	\item Визуализируйте взаимоотношения всех числовых характеристих - покажите их взаимное влияние на графике. 
	\item Реализуйте алгоритм CART с друмя метриками:
      \begin{enumerate}
          \item индексом Gini,
          \item и критерием Twoing,
          \item можете реализовать ещё метрики, это будет отмечено дополнительными баллами.
      \end{enumerate}
Посмотрите, как работают эти критерии при разных параметрах построения дерева. Описание алгоритма CART можно посмотреть \href{ftp://public.dhe.ibm.com/software/analytics/spss/support/Stats/Docs/Statistics/Algorithms/13.0/TREE-CART.pdf}{здесь}.
    
    \item (По желанию.) Реализуйте прунинг, описание алгоритма есть в ноутбуке, а дополнительное описание можно посмотреть \href{http://www.dcc.fc.up.pt/~ltorgo/PhD/th4.pdf}{здесь}.
    В некоторых источниках к прунингу относят критерии останова роста дерева, так что в этой терминологии задание - реализовать post-pruning. 
    \item Визуализируйте ваше получившееся после обучения на датасете дерево (с помощью GraphViz).
    \item Визуализируйте эффективность решения разных вариаций решающего дерева, сравните их на одной картинке. Дополнительно можно сравнить с классификатором на основе kNN.
    \item Реализуйте алгоритм построения леса. Для этого вам нужно реализовать процедуру бэггинга. Подробнее на эту тему пожно прочитать в книге \href{https://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf}{Elements of Statistical Learning}, c. 282 и далее.
    \item \textbf{Задание считается сданным после отправки ipython notebook, с описанием и кодом проведенных экспериментов, наглядными графиками и правильными выводами} 
\end{enumerate}

\subsection*{Методические указания}
  \begin{enumerate}
  \item При подборе параметров модели рекомендуется использовать только часть обучающей выборки, для того чтобы сократить время обучения. 
  \item Согласно правилам соревнований нельзя делать больше 3х коммитов в систему в сутки. Из этого надо сделать следующие выводы:
  \begin{enumerate}
      \item Обучаться нужно локально (cross-validation) и только после получения результата, который вы считаете удовлетворительным, нужно делать submit в систему.
      \item Начать делать домашнее задание стоит заблаговременно. 
  \end{enumerate}
  \item Обратите внимание, что публичные результаты на kaggle рассчитываются только по части контрольной выборки, и будут рассчитаны по всей контрольной выборке после окончания соревнования. Будьте аккуратны с переобучением.  
  \item Победители получают бонусные балы -- шарить решение не выгодно.
\end{enumerate}
\vspace{12cm}
\emph{Разница между списыванием и помощью товарища иногда едва различима. Мы искренне надеемся, что при любых сложностях вы можете обратиться к семинаристам и с их подсказками самостоятельно справиться с заданием. При зафиксированных случаях списывания (одинаковый код, решение задачи), баллы за задание будут обнулены всем участникам инцидента.}

\end{document}